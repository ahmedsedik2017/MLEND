{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Nanodegree\n",
    "## Capstone Project : Iris Recognition Using CNN\n",
    "### Ahmed Sedik Al-Ads\n",
    "\n",
    "In this project we will implement the proposal that had been reviewed. This project evaluates the accuracy for both of benchmark and our proposed CNN models. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Import Datasets\n",
    "\n",
    "**In the code cell below, we import a dataset of iris images.  We populate a few variables through the use of the `load_files` function from the scikit-learn library:**\n",
    "\n",
    "- `train_files`, `valid_files`, `test_files` - numpy arrays containing file paths to images\n",
    "- `train_targets`, `valid_targets`, `test_targets` - numpy arrays containing onehot-encoded classification labels \n",
    "- `cat_names` - list of string-valued person's folder names for translating labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 26 total Iris categories.\n",
      "There are 1300 total Iris images.\n",
      "\n",
      "There are 1040 training Iris images.\n",
      "There are 260 test Iris images.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_files       \n",
    "from keras.utils import np_utils\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "\n",
    "# define function to load train, test, and validation datasets\n",
    "def load_dataset(path):\n",
    "    data = load_files(path)\n",
    "    iris_files = np.array(data['filenames'])\n",
    "    iris_targets = np_utils.to_categorical(np.array(data['target']), 26)\n",
    "    return iris_files, iris_targets\n",
    "\n",
    "# load train, test, and validation datasets\n",
    "train_files, train_targets = load_dataset('casia/train')\n",
    "valid_files, valid_targets = load_dataset('casia/test')\n",
    "\n",
    "test_files, test_targets = load_dataset('casia/test')\n",
    "\n",
    "# load list of dog names\n",
    "cat_names = [item[20:-1] for item in sorted(glob(\"casia/train/*/\"))]\n",
    "\n",
    "# print statistics about the dataset\n",
    "print('There are %d total Iris categories.' % len(cat_names))\n",
    "print('There are %s total Iris images.\\n' % len(np.hstack([train_files, test_files])))\n",
    "print('There are %d training Iris images.' % len(train_files))\n",
    "\n",
    "print('There are %d test Iris images.'% len(test_files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Pre-processing\n",
    "\n",
    "\n",
    "**When using TensorFlow as backend, Keras CNNs require a 4D array (which we'll also refer to as a 4D tensor) as input, with shape**\n",
    "\n",
    "$$\n",
    "(\\text{nb_samples}, \\text{rows}, \\text{columns}, \\text{channels}),\n",
    "$$\n",
    "\n",
    "where `nb_samples` corresponds to the total number of images (or samples), and `rows`, `columns`, and `channels` correspond to the number of rows, columns, and channels for each image, respectively.  \n",
    "\n",
    "The `path_to_tensor` function below takes a string-valued file path to a color image as input and returns a 4D tensor suitable for supplying to a Keras CNN.  The function first loads the image and resizes it to a square image that is $224 \\times 224$ pixels.  Next, the image is converted to an array, which is then resized to a 4D tensor.  In this case, since we are working with color images, each image has three channels.  Likewise, since we are processing a single image (or sample), the returned tensor will always have shape\n",
    "\n",
    "$$\n",
    "(1, 224, 224, 3).\n",
    "$$\n",
    "\n",
    "The `paths_to_tensor` function takes a numpy array of string-valued image paths as input and returns a 4D tensor with shape \n",
    "\n",
    "$$\n",
    "(\\text{nb_samples}, 224, 224, 3).\n",
    "$$\n",
    "\n",
    "Here, `nb_samples` is the number of samples, or number of images, in the supplied array of image paths.  It is best to think of `nb_samples` as the number of 3D tensors (where each 3D tensor corresponds to a different image) in your dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import image                  \n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def path_to_tensor(img_path):\n",
    "    # loads RGB image as PIL.Image.Image type\n",
    "    img = image.load_img(img_path, target_size=(224, 224))\n",
    "    # convert PIL.Image.Image type to 3D tensor with shape (224, 224, 3)\n",
    "    x = image.img_to_array(img)\n",
    "    # convert 3D tensor to 4D tensor with shape (1, 224, 224, 3) and return 4D tensor\n",
    "    return np.expand_dims(x, axis=0)\n",
    "\n",
    "def paths_to_tensor(img_paths):\n",
    "    list_of_tensors = [path_to_tensor(img_path) for img_path in tqdm(img_paths)]\n",
    "    return np.vstack(list_of_tensors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-process the Data\n",
    "\n",
    "We rescale the images by dividing every pixel in every image by 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 1040/1040 [00:04<00:00, 238.92it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 260/260 [00:01<00:00, 242.59it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 260/260 [00:00<00:00, 263.46it/s]\n"
     ]
    }
   ],
   "source": [
    "from PIL import ImageFile                            \n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True                 \n",
    "\n",
    "# pre-process the data for Keras\n",
    "train_tensors = paths_to_tensor(train_files).astype('float32')/255\n",
    "valid_tensors = paths_to_tensor(valid_files).astype('float32')/255\n",
    "\n",
    "test_tensors = paths_to_tensor(test_files).astype('float32')/255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Benchmark Model\n",
    "\n",
    "This CNN consists of one CNV layer with 32 filters followed by one max pooling layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "batch_normalization_1 (Batch (None, 224, 224, 3)       12        \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 222, 222, 32)      896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 111, 111, 32)      0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 111, 111, 32)      128       \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_1 ( (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 26)                858       \n",
      "=================================================================\n",
      "Total params: 1,894\n",
      "Trainable params: 1,824\n",
      "Non-trainable params: 70\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D\n",
    "from keras.layers import Dropout, Flatten, Dense, Activation\n",
    "from keras.models import Sequential\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "\n",
    "\n",
    "model0 = Sequential()\n",
    "model0.add(BatchNormalization(input_shape=(224, 224, 3)))\n",
    "model0.add(Conv2D(filters=32, kernel_size=3, kernel_initializer='he_normal', activation='relu'))\n",
    "model0.add(MaxPooling2D(pool_size=2))\n",
    "model0.add(BatchNormalization())\n",
    "\n",
    "\n",
    "model0.add(GlobalAveragePooling2D())\n",
    "\n",
    "model0.add(Dense(26, activation='softmax'))\n",
    "\n",
    "model0.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model0.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Model\n",
    "\n",
    "* No. of Epoches = 10\n",
    "\n",
    "* We will save the model using checkpointer0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1040 samples, validate on 260 samples\n",
      "Epoch 1/10\n",
      "1020/1040 [============================>.] - ETA: 2:45 - loss: 3.2589 - acc: 0.100 - ETA: 2:23 - loss: 3.2558 - acc: 0.050 - ETA: 2:13 - loss: 3.2476 - acc: 0.033 - ETA: 2:07 - loss: 3.2583 - acc: 0.025 - ETA: 2:02 - loss: 3.2522 - acc: 0.030 - ETA: 1:59 - loss: 3.2464 - acc: 0.075 - ETA: 1:55 - loss: 3.2405 - acc: 0.092 - ETA: 1:52 - loss: 3.2363 - acc: 0.093 - ETA: 1:49 - loss: 3.2364 - acc: 0.100 - ETA: 1:46 - loss: 3.2321 - acc: 0.110 - ETA: 1:43 - loss: 3.2312 - acc: 0.104 - ETA: 1:40 - loss: 3.2294 - acc: 0.116 - ETA: 1:38 - loss: 3.2317 - acc: 0.111 - ETA: 1:35 - loss: 3.2285 - acc: 0.114 - ETA: 1:33 - loss: 3.2261 - acc: 0.113 - ETA: 1:30 - loss: 3.2253 - acc: 0.112 - ETA: 1:27 - loss: 3.2230 - acc: 0.114 - ETA: 1:25 - loss: 3.2226 - acc: 0.113 - ETA: 1:22 - loss: 3.2217 - acc: 0.113 - ETA: 1:19 - loss: 3.2203 - acc: 0.110 - ETA: 1:17 - loss: 3.2198 - acc: 0.104 - ETA: 1:14 - loss: 3.2146 - acc: 0.111 - ETA: 1:11 - loss: 3.2148 - acc: 0.110 - ETA: 1:09 - loss: 3.2150 - acc: 0.110 - ETA: 1:07 - loss: 3.2140 - acc: 0.108 - ETA: 1:04 - loss: 3.2128 - acc: 0.111 - ETA: 1:02 - loss: 3.2114 - acc: 0.111 - ETA: 59s - loss: 3.2116 - acc: 0.107 - ETA: 57s - loss: 3.2112 - acc: 0.11 - ETA: 54s - loss: 3.2106 - acc: 0.10 - ETA: 52s - loss: 3.2098 - acc: 0.10 - ETA: 50s - loss: 3.2094 - acc: 0.10 - ETA: 47s - loss: 3.2093 - acc: 0.10 - ETA: 45s - loss: 3.2075 - acc: 0.11 - ETA: 42s - loss: 3.2054 - acc: 0.11 - ETA: 40s - loss: 3.2040 - acc: 0.11 - ETA: 37s - loss: 3.2041 - acc: 0.11 - ETA: 35s - loss: 3.2035 - acc: 0.11 - ETA: 32s - loss: 3.2031 - acc: 0.11 - ETA: 30s - loss: 3.2019 - acc: 0.11 - ETA: 27s - loss: 3.2012 - acc: 0.11 - ETA: 25s - loss: 3.1994 - acc: 0.11 - ETA: 22s - loss: 3.1972 - acc: 0.11 - ETA: 20s - loss: 3.1967 - acc: 0.12 - ETA: 17s - loss: 3.1955 - acc: 0.12 - ETA: 15s - loss: 3.1958 - acc: 0.11 - ETA: 12s - loss: 3.1943 - acc: 0.12 - ETA: 10s - loss: 3.1942 - acc: 0.11 - ETA: 7s - loss: 3.1929 - acc: 0.1224 - ETA: 5s - loss: 3.1920 - acc: 0.123 - ETA: 2s - loss: 3.1915 - acc: 0.1225Epoch 00001: val_loss improved from inf to 3.27491, saving model to saved_models/weights.best.benchmark_Model.hdf5\n",
      "1040/1040 [==============================] - 154s 148ms/step - loss: 3.1920 - acc: 0.1221 - val_loss: 3.2749 - val_acc: 0.0385\n",
      "Epoch 2/10\n",
      "1020/1040 [============================>.] - ETA: 2:47 - loss: 3.1227 - acc: 0.200 - ETA: 2:36 - loss: 3.1493 - acc: 0.200 - ETA: 2:21 - loss: 3.1475 - acc: 0.166 - ETA: 2:13 - loss: 3.1455 - acc: 0.187 - ETA: 2:13 - loss: 3.1462 - acc: 0.160 - ETA: 2:12 - loss: 3.1453 - acc: 0.166 - ETA: 2:06 - loss: 3.1415 - acc: 0.164 - ETA: 2:01 - loss: 3.1350 - acc: 0.181 - ETA: 2:01 - loss: 3.1289 - acc: 0.172 - ETA: 1:58 - loss: 3.1331 - acc: 0.170 - ETA: 1:54 - loss: 3.1331 - acc: 0.172 - ETA: 1:50 - loss: 3.1343 - acc: 0.162 - ETA: 1:50 - loss: 3.1343 - acc: 0.153 - ETA: 1:47 - loss: 3.1315 - acc: 0.160 - ETA: 1:43 - loss: 3.1306 - acc: 0.160 - ETA: 1:41 - loss: 3.1331 - acc: 0.153 - ETA: 1:39 - loss: 3.1341 - acc: 0.155 - ETA: 1:35 - loss: 3.1339 - acc: 0.161 - ETA: 1:32 - loss: 3.1341 - acc: 0.155 - ETA: 1:30 - loss: 3.1363 - acc: 0.152 - ETA: 1:27 - loss: 3.1398 - acc: 0.157 - ETA: 1:24 - loss: 3.1378 - acc: 0.156 - ETA: 1:22 - loss: 3.1385 - acc: 0.154 - ETA: 1:19 - loss: 3.1402 - acc: 0.154 - ETA: 1:16 - loss: 3.1385 - acc: 0.154 - ETA: 1:14 - loss: 3.1370 - acc: 0.153 - ETA: 1:11 - loss: 3.1359 - acc: 0.155 - ETA: 1:08 - loss: 3.1350 - acc: 0.157 - ETA: 1:05 - loss: 3.1328 - acc: 0.163 - ETA: 1:02 - loss: 3.1342 - acc: 0.163 - ETA: 59s - loss: 3.1349 - acc: 0.161 - ETA: 56s - loss: 3.1344 - acc: 0.16 - ETA: 54s - loss: 3.1320 - acc: 0.16 - ETA: 51s - loss: 3.1313 - acc: 0.17 - ETA: 48s - loss: 3.1305 - acc: 0.17 - ETA: 45s - loss: 3.1321 - acc: 0.16 - ETA: 42s - loss: 3.1340 - acc: 0.16 - ETA: 39s - loss: 3.1346 - acc: 0.16 - ETA: 36s - loss: 3.1321 - acc: 0.16 - ETA: 34s - loss: 3.1316 - acc: 0.16 - ETA: 31s - loss: 3.1320 - acc: 0.16 - ETA: 28s - loss: 3.1313 - acc: 0.16 - ETA: 25s - loss: 3.1299 - acc: 0.16 - ETA: 22s - loss: 3.1283 - acc: 0.16 - ETA: 19s - loss: 3.1283 - acc: 0.16 - ETA: 16s - loss: 3.1273 - acc: 0.16 - ETA: 14s - loss: 3.1275 - acc: 0.16 - ETA: 11s - loss: 3.1277 - acc: 0.16 - ETA: 8s - loss: 3.1267 - acc: 0.1653 - ETA: 5s - loss: 3.1273 - acc: 0.163 - ETA: 2s - loss: 3.1274 - acc: 0.1637Epoch 00002: val_loss improved from 3.27491 to 3.26458, saving model to saved_models/weights.best.benchmark_Model.hdf5\n",
      "1040/1040 [==============================] - 169s 162ms/step - loss: 3.1259 - acc: 0.1635 - val_loss: 3.2646 - val_acc: 0.0692\n",
      "Epoch 3/10\n",
      "1020/1040 [============================>.] - ETA: 3:16 - loss: 3.1229 - acc: 0.100 - ETA: 2:37 - loss: 3.1021 - acc: 0.150 - ETA: 2:25 - loss: 3.0980 - acc: 0.183 - ETA: 2:26 - loss: 3.0896 - acc: 0.162 - ETA: 2:22 - loss: 3.0843 - acc: 0.160 - ETA: 2:15 - loss: 3.0935 - acc: 0.150 - ETA: 2:13 - loss: 3.0962 - acc: 0.157 - ETA: 2:11 - loss: 3.0950 - acc: 0.156 - ETA: 2:07 - loss: 3.0978 - acc: 0.155 - ETA: 2:08 - loss: 3.1030 - acc: 0.160 - ETA: 2:02 - loss: 3.0987 - acc: 0.159 - ETA: 1:58 - loss: 3.0975 - acc: 0.162 - ETA: 1:57 - loss: 3.0960 - acc: 0.165 - ETA: 1:53 - loss: 3.0963 - acc: 0.167 - ETA: 1:48 - loss: 3.0938 - acc: 0.163 - ETA: 1:46 - loss: 3.0950 - acc: 0.165 - ETA: 1:43 - loss: 3.0942 - acc: 0.164 - ETA: 1:39 - loss: 3.0903 - acc: 0.166 - ETA: 1:37 - loss: 3.0896 - acc: 0.165 - ETA: 1:35 - loss: 3.0878 - acc: 0.170 - ETA: 1:33 - loss: 3.0870 - acc: 0.169 - ETA: 1:30 - loss: 3.0841 - acc: 0.179 - ETA: 1:29 - loss: 3.0799 - acc: 0.187 - ETA: 1:25 - loss: 3.0803 - acc: 0.183 - ETA: 1:23 - loss: 3.0826 - acc: 0.180 - ETA: 1:21 - loss: 3.0819 - acc: 0.178 - ETA: 1:17 - loss: 3.0788 - acc: 0.183 - ETA: 1:14 - loss: 3.0774 - acc: 0.185 - ETA: 1:11 - loss: 3.0793 - acc: 0.181 - ETA: 1:08 - loss: 3.0806 - acc: 0.181 - ETA: 1:05 - loss: 3.0780 - acc: 0.180 - ETA: 1:02 - loss: 3.0772 - acc: 0.181 - ETA: 58s - loss: 3.0761 - acc: 0.180 - ETA: 55s - loss: 3.0754 - acc: 0.18 - ETA: 52s - loss: 3.0713 - acc: 0.18 - ETA: 49s - loss: 3.0728 - acc: 0.18 - ETA: 46s - loss: 3.0708 - acc: 0.18 - ETA: 43s - loss: 3.0713 - acc: 0.18 - ETA: 39s - loss: 3.0703 - acc: 0.18 - ETA: 36s - loss: 3.0686 - acc: 0.18 - ETA: 33s - loss: 3.0687 - acc: 0.18 - ETA: 30s - loss: 3.0648 - acc: 0.18 - ETA: 27s - loss: 3.0639 - acc: 0.18 - ETA: 24s - loss: 3.0650 - acc: 0.18 - ETA: 21s - loss: 3.0626 - acc: 0.18 - ETA: 18s - loss: 3.0617 - acc: 0.18 - ETA: 15s - loss: 3.0621 - acc: 0.18 - ETA: 12s - loss: 3.0627 - acc: 0.18 - ETA: 9s - loss: 3.0634 - acc: 0.1837 - ETA: 6s - loss: 3.0634 - acc: 0.182 - ETA: 3s - loss: 3.0649 - acc: 0.1804Epoch 00003: val_loss did not improve\n",
      "1040/1040 [==============================] - 180s 173ms/step - loss: 3.0650 - acc: 0.1798 - val_loss: 3.2661 - val_acc: 0.0346\n",
      "Epoch 4/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1020/1040 [============================>.] - ETA: 3:15 - loss: 2.9005 - acc: 0.250 - ETA: 2:36 - loss: 2.9747 - acc: 0.175 - ETA: 2:23 - loss: 2.9824 - acc: 0.233 - ETA: 2:22 - loss: 2.9829 - acc: 0.200 - ETA: 2:22 - loss: 3.0021 - acc: 0.200 - ETA: 2:14 - loss: 3.0085 - acc: 0.191 - ETA: 2:13 - loss: 3.0120 - acc: 0.185 - ETA: 2:11 - loss: 3.0211 - acc: 0.187 - ETA: 2:06 - loss: 3.0279 - acc: 0.177 - ETA: 2:04 - loss: 3.0372 - acc: 0.170 - ETA: 2:02 - loss: 3.0368 - acc: 0.172 - ETA: 1:57 - loss: 3.0325 - acc: 0.179 - ETA: 1:55 - loss: 3.0317 - acc: 0.184 - ETA: 1:52 - loss: 3.0296 - acc: 0.189 - ETA: 1:48 - loss: 3.0305 - acc: 0.196 - ETA: 1:46 - loss: 3.0308 - acc: 0.190 - ETA: 1:43 - loss: 3.0318 - acc: 0.188 - ETA: 1:39 - loss: 3.0354 - acc: 0.183 - ETA: 1:37 - loss: 3.0392 - acc: 0.176 - ETA: 1:34 - loss: 3.0356 - acc: 0.177 - ETA: 1:31 - loss: 3.0385 - acc: 0.181 - ETA: 1:28 - loss: 3.0383 - acc: 0.186 - ETA: 1:25 - loss: 3.0344 - acc: 0.189 - ETA: 1:22 - loss: 3.0357 - acc: 0.183 - ETA: 1:19 - loss: 3.0301 - acc: 0.188 - ETA: 1:16 - loss: 3.0308 - acc: 0.188 - ETA: 1:13 - loss: 3.0284 - acc: 0.190 - ETA: 1:10 - loss: 3.0282 - acc: 0.189 - ETA: 1:07 - loss: 3.0294 - acc: 0.186 - ETA: 1:04 - loss: 3.0298 - acc: 0.185 - ETA: 1:01 - loss: 3.0278 - acc: 0.185 - ETA: 59s - loss: 3.0273 - acc: 0.187 - ETA: 55s - loss: 3.0271 - acc: 0.18 - ETA: 53s - loss: 3.0249 - acc: 0.18 - ETA: 50s - loss: 3.0219 - acc: 0.19 - ETA: 47s - loss: 3.0196 - acc: 0.19 - ETA: 44s - loss: 3.0189 - acc: 0.19 - ETA: 41s - loss: 3.0170 - acc: 0.18 - ETA: 38s - loss: 3.0136 - acc: 0.18 - ETA: 35s - loss: 3.0143 - acc: 0.19 - ETA: 32s - loss: 3.0120 - acc: 0.18 - ETA: 29s - loss: 3.0101 - acc: 0.19 - ETA: 26s - loss: 3.0095 - acc: 0.18 - ETA: 23s - loss: 3.0104 - acc: 0.19 - ETA: 20s - loss: 3.0108 - acc: 0.19 - ETA: 17s - loss: 3.0111 - acc: 0.19 - ETA: 14s - loss: 3.0110 - acc: 0.18 - ETA: 11s - loss: 3.0121 - acc: 0.18 - ETA: 8s - loss: 3.0097 - acc: 0.1878 - ETA: 5s - loss: 3.0092 - acc: 0.189 - ETA: 2s - loss: 3.0093 - acc: 0.1863Epoch 00004: val_loss did not improve\n",
      "1040/1040 [==============================] - 175s 168ms/step - loss: 3.0096 - acc: 0.1875 - val_loss: 3.2937 - val_acc: 0.0385\n",
      "Epoch 5/10\n",
      "1020/1040 [============================>.] - ETA: 2:02 - loss: 3.0068 - acc: 0.300 - ETA: 2:18 - loss: 3.0081 - acc: 0.250 - ETA: 2:23 - loss: 3.0274 - acc: 0.183 - ETA: 2:14 - loss: 3.0027 - acc: 0.237 - ETA: 2:13 - loss: 3.0083 - acc: 0.190 - ETA: 2:14 - loss: 2.9849 - acc: 0.225 - ETA: 2:08 - loss: 2.9696 - acc: 0.242 - ETA: 2:06 - loss: 2.9675 - acc: 0.243 - ETA: 2:06 - loss: 2.9732 - acc: 0.233 - ETA: 2:01 - loss: 2.9611 - acc: 0.250 - ETA: 1:58 - loss: 2.9573 - acc: 0.240 - ETA: 1:57 - loss: 2.9605 - acc: 0.237 - ETA: 1:53 - loss: 2.9636 - acc: 0.238 - ETA: 1:50 - loss: 2.9671 - acc: 0.232 - ETA: 1:49 - loss: 2.9658 - acc: 0.233 - ETA: 1:44 - loss: 2.9641 - acc: 0.231 - ETA: 1:42 - loss: 2.9634 - acc: 0.235 - ETA: 1:40 - loss: 2.9610 - acc: 0.238 - ETA: 1:36 - loss: 2.9649 - acc: 0.236 - ETA: 1:33 - loss: 2.9704 - acc: 0.230 - ETA: 1:31 - loss: 2.9621 - acc: 0.233 - ETA: 1:27 - loss: 2.9658 - acc: 0.229 - ETA: 1:23 - loss: 2.9633 - acc: 0.230 - ETA: 1:21 - loss: 2.9632 - acc: 0.229 - ETA: 1:18 - loss: 2.9600 - acc: 0.230 - ETA: 1:15 - loss: 2.9649 - acc: 0.226 - ETA: 1:12 - loss: 2.9655 - acc: 0.227 - ETA: 1:10 - loss: 2.9663 - acc: 0.226 - ETA: 1:06 - loss: 2.9676 - acc: 0.225 - ETA: 1:03 - loss: 2.9688 - acc: 0.223 - ETA: 1:01 - loss: 2.9683 - acc: 0.224 - ETA: 58s - loss: 2.9650 - acc: 0.220 - ETA: 55s - loss: 2.9672 - acc: 0.21 - ETA: 52s - loss: 2.9697 - acc: 0.21 - ETA: 49s - loss: 2.9663 - acc: 0.22 - ETA: 46s - loss: 2.9687 - acc: 0.21 - ETA: 43s - loss: 2.9691 - acc: 0.21 - ETA: 41s - loss: 2.9679 - acc: 0.21 - ETA: 38s - loss: 2.9667 - acc: 0.21 - ETA: 35s - loss: 2.9637 - acc: 0.22 - ETA: 32s - loss: 2.9625 - acc: 0.22 - ETA: 29s - loss: 2.9603 - acc: 0.22 - ETA: 26s - loss: 2.9588 - acc: 0.22 - ETA: 23s - loss: 2.9558 - acc: 0.22 - ETA: 20s - loss: 2.9584 - acc: 0.22 - ETA: 17s - loss: 2.9556 - acc: 0.22 - ETA: 14s - loss: 2.9547 - acc: 0.22 - ETA: 11s - loss: 2.9561 - acc: 0.22 - ETA: 8s - loss: 2.9568 - acc: 0.2245 - ETA: 5s - loss: 2.9568 - acc: 0.225 - ETA: 2s - loss: 2.9591 - acc: 0.2245Epoch 00005: val_loss did not improve\n",
      "1040/1040 [==============================] - 175s 168ms/step - loss: 2.9589 - acc: 0.2240 - val_loss: 3.2930 - val_acc: 0.0731\n",
      "Epoch 6/10\n",
      "1020/1040 [============================>.] - ETA: 3:09 - loss: 2.9422 - acc: 0.200 - ETA: 2:34 - loss: 2.9684 - acc: 0.225 - ETA: 2:41 - loss: 3.0043 - acc: 0.200 - ETA: 2:27 - loss: 2.9870 - acc: 0.200 - ETA: 2:24 - loss: 2.9891 - acc: 0.200 - ETA: 2:22 - loss: 2.9909 - acc: 0.175 - ETA: 2:14 - loss: 2.9665 - acc: 0.200 - ETA: 2:12 - loss: 2.9609 - acc: 0.206 - ETA: 2:10 - loss: 2.9650 - acc: 0.211 - ETA: 2:05 - loss: 2.9580 - acc: 0.205 - ETA: 2:00 - loss: 2.9647 - acc: 0.195 - ETA: 1:57 - loss: 2.9532 - acc: 0.212 - ETA: 1:55 - loss: 2.9588 - acc: 0.215 - ETA: 1:51 - loss: 2.9550 - acc: 0.203 - ETA: 1:48 - loss: 2.9512 - acc: 0.203 - ETA: 1:46 - loss: 2.9502 - acc: 0.200 - ETA: 1:42 - loss: 2.9497 - acc: 0.197 - ETA: 1:39 - loss: 2.9433 - acc: 0.205 - ETA: 1:37 - loss: 2.9334 - acc: 0.215 - ETA: 1:33 - loss: 2.9349 - acc: 0.212 - ETA: 1:31 - loss: 2.9304 - acc: 0.214 - ETA: 1:28 - loss: 2.9267 - acc: 0.220 - ETA: 1:24 - loss: 2.9283 - acc: 0.221 - ETA: 1:22 - loss: 2.9269 - acc: 0.222 - ETA: 1:19 - loss: 2.9266 - acc: 0.222 - ETA: 1:16 - loss: 2.9309 - acc: 0.219 - ETA: 1:13 - loss: 2.9279 - acc: 0.224 - ETA: 1:10 - loss: 2.9276 - acc: 0.225 - ETA: 1:07 - loss: 2.9273 - acc: 0.222 - ETA: 1:05 - loss: 2.9270 - acc: 0.223 - ETA: 1:01 - loss: 2.9274 - acc: 0.225 - ETA: 58s - loss: 2.9213 - acc: 0.232 - ETA: 55s - loss: 2.9261 - acc: 0.22 - ETA: 53s - loss: 2.9242 - acc: 0.23 - ETA: 49s - loss: 2.9223 - acc: 0.23 - ETA: 47s - loss: 2.9258 - acc: 0.23 - ETA: 44s - loss: 2.9291 - acc: 0.22 - ETA: 42s - loss: 2.9292 - acc: 0.22 - ETA: 39s - loss: 2.9328 - acc: 0.22 - ETA: 36s - loss: 2.9267 - acc: 0.22 - ETA: 33s - loss: 2.9298 - acc: 0.22 - ETA: 30s - loss: 2.9290 - acc: 0.22 - ETA: 27s - loss: 2.9297 - acc: 0.22 - ETA: 24s - loss: 2.9281 - acc: 0.22 - ETA: 21s - loss: 2.9250 - acc: 0.22 - ETA: 18s - loss: 2.9263 - acc: 0.22 - ETA: 15s - loss: 2.9257 - acc: 0.22 - ETA: 12s - loss: 2.9251 - acc: 0.22 - ETA: 9s - loss: 2.9232 - acc: 0.2235 - ETA: 6s - loss: 2.9216 - acc: 0.224 - ETA: 3s - loss: 2.9216 - acc: 0.2265Epoch 00006: val_loss did not improve\n",
      "1040/1040 [==============================] - 180s 173ms/step - loss: 2.9191 - acc: 0.2279 - val_loss: 3.2828 - val_acc: 0.0731\n",
      "Epoch 7/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1020/1040 [============================>.] - ETA: 3:14 - loss: 2.8103 - acc: 0.350 - ETA: 2:38 - loss: 2.8266 - acc: 0.375 - ETA: 2:23 - loss: 2.8039 - acc: 0.383 - ETA: 2:23 - loss: 2.8488 - acc: 0.325 - ETA: 2:22 - loss: 2.8516 - acc: 0.320 - ETA: 2:17 - loss: 2.8662 - acc: 0.275 - ETA: 2:20 - loss: 2.8612 - acc: 0.271 - ETA: 2:17 - loss: 2.8678 - acc: 0.268 - ETA: 2:14 - loss: 2.8795 - acc: 0.250 - ETA: 2:09 - loss: 2.8749 - acc: 0.255 - ETA: 2:08 - loss: 2.8628 - acc: 0.250 - ETA: 2:03 - loss: 2.8556 - acc: 0.258 - ETA: 1:58 - loss: 2.8689 - acc: 0.242 - ETA: 1:57 - loss: 2.8698 - acc: 0.239 - ETA: 1:52 - loss: 2.8773 - acc: 0.233 - ETA: 1:51 - loss: 2.8723 - acc: 0.243 - ETA: 1:46 - loss: 2.8671 - acc: 0.244 - ETA: 1:43 - loss: 2.8672 - acc: 0.238 - ETA: 1:40 - loss: 2.8654 - acc: 0.239 - ETA: 1:36 - loss: 2.8729 - acc: 0.237 - ETA: 1:33 - loss: 2.8731 - acc: 0.238 - ETA: 1:30 - loss: 2.8784 - acc: 0.243 - ETA: 1:27 - loss: 2.8732 - acc: 0.252 - ETA: 1:24 - loss: 2.8726 - acc: 0.254 - ETA: 1:21 - loss: 2.8740 - acc: 0.254 - ETA: 1:18 - loss: 2.8699 - acc: 0.261 - ETA: 1:15 - loss: 2.8711 - acc: 0.261 - ETA: 1:12 - loss: 2.8650 - acc: 0.267 - ETA: 1:09 - loss: 2.8657 - acc: 0.265 - ETA: 1:06 - loss: 2.8681 - acc: 0.260 - ETA: 1:03 - loss: 2.8765 - acc: 0.254 - ETA: 1:00 - loss: 2.8823 - acc: 0.251 - ETA: 57s - loss: 2.8821 - acc: 0.251 - ETA: 54s - loss: 2.8854 - acc: 0.24 - ETA: 51s - loss: 2.8844 - acc: 0.24 - ETA: 48s - loss: 2.8848 - acc: 0.24 - ETA: 45s - loss: 2.8873 - acc: 0.24 - ETA: 42s - loss: 2.8844 - acc: 0.24 - ETA: 39s - loss: 2.8825 - acc: 0.25 - ETA: 36s - loss: 2.8821 - acc: 0.25 - ETA: 33s - loss: 2.8842 - acc: 0.24 - ETA: 30s - loss: 2.8834 - acc: 0.24 - ETA: 27s - loss: 2.8801 - acc: 0.25 - ETA: 24s - loss: 2.8786 - acc: 0.25 - ETA: 21s - loss: 2.8775 - acc: 0.25 - ETA: 18s - loss: 2.8745 - acc: 0.25 - ETA: 15s - loss: 2.8724 - acc: 0.25 - ETA: 12s - loss: 2.8707 - acc: 0.25 - ETA: 9s - loss: 2.8699 - acc: 0.2571 - ETA: 6s - loss: 2.8708 - acc: 0.255 - ETA: 3s - loss: 2.8675 - acc: 0.2559Epoch 00007: val_loss improved from 3.26458 to 3.20457, saving model to saved_models/weights.best.benchmark_Model.hdf5\n",
      "1040/1040 [==============================] - 178s 171ms/step - loss: 2.8705 - acc: 0.2538 - val_loss: 3.2046 - val_acc: 0.0731\n",
      "Epoch 8/10\n",
      "1020/1040 [============================>.] - ETA: 3:11 - loss: 2.7736 - acc: 0.350 - ETA: 2:34 - loss: 2.8137 - acc: 0.275 - ETA: 2:24 - loss: 2.8013 - acc: 0.283 - ETA: 2:30 - loss: 2.8268 - acc: 0.262 - ETA: 2:20 - loss: 2.8092 - acc: 0.290 - ETA: 2:22 - loss: 2.8395 - acc: 0.275 - ETA: 2:15 - loss: 2.8349 - acc: 0.242 - ETA: 2:15 - loss: 2.8212 - acc: 0.256 - ETA: 2:10 - loss: 2.8260 - acc: 0.255 - ETA: 2:04 - loss: 2.8115 - acc: 0.265 - ETA: 2:04 - loss: 2.8132 - acc: 0.268 - ETA: 2:02 - loss: 2.8135 - acc: 0.275 - ETA: 1:57 - loss: 2.8128 - acc: 0.273 - ETA: 1:54 - loss: 2.8188 - acc: 0.264 - ETA: 1:52 - loss: 2.8241 - acc: 0.256 - ETA: 1:47 - loss: 2.8292 - acc: 0.246 - ETA: 1:44 - loss: 2.8327 - acc: 0.250 - ETA: 1:42 - loss: 2.8451 - acc: 0.238 - ETA: 1:38 - loss: 2.8445 - acc: 0.242 - ETA: 1:35 - loss: 2.8396 - acc: 0.245 - ETA: 1:33 - loss: 2.8408 - acc: 0.242 - ETA: 1:29 - loss: 2.8438 - acc: 0.245 - ETA: 1:26 - loss: 2.8482 - acc: 0.241 - ETA: 1:23 - loss: 2.8534 - acc: 0.237 - ETA: 1:20 - loss: 2.8498 - acc: 0.242 - ETA: 1:17 - loss: 2.8478 - acc: 0.244 - ETA: 1:14 - loss: 2.8507 - acc: 0.240 - ETA: 1:11 - loss: 2.8478 - acc: 0.244 - ETA: 1:08 - loss: 2.8459 - acc: 0.248 - ETA: 1:05 - loss: 2.8454 - acc: 0.251 - ETA: 1:02 - loss: 2.8404 - acc: 0.254 - ETA: 59s - loss: 2.8424 - acc: 0.254 - ETA: 56s - loss: 2.8377 - acc: 0.25 - ETA: 53s - loss: 2.8393 - acc: 0.25 - ETA: 50s - loss: 2.8358 - acc: 0.25 - ETA: 47s - loss: 2.8377 - acc: 0.24 - ETA: 44s - loss: 2.8331 - acc: 0.25 - ETA: 41s - loss: 2.8328 - acc: 0.25 - ETA: 38s - loss: 2.8305 - acc: 0.25 - ETA: 35s - loss: 2.8339 - acc: 0.25 - ETA: 32s - loss: 2.8350 - acc: 0.25 - ETA: 29s - loss: 2.8319 - acc: 0.25 - ETA: 26s - loss: 2.8306 - acc: 0.26 - ETA: 23s - loss: 2.8330 - acc: 0.25 - ETA: 20s - loss: 2.8340 - acc: 0.25 - ETA: 17s - loss: 2.8327 - acc: 0.25 - ETA: 14s - loss: 2.8317 - acc: 0.25 - ETA: 11s - loss: 2.8316 - acc: 0.25 - ETA: 8s - loss: 2.8305 - acc: 0.2541 - ETA: 5s - loss: 2.8294 - acc: 0.256 - ETA: 2s - loss: 2.8284 - acc: 0.2569Epoch 00008: val_loss improved from 3.20457 to 3.10371, saving model to saved_models/weights.best.benchmark_Model.hdf5\n",
      "1040/1040 [==============================] - 177s 170ms/step - loss: 2.8268 - acc: 0.2577 - val_loss: 3.1037 - val_acc: 0.1154\n",
      "Epoch 9/10\n",
      "1020/1040 [============================>.] - ETA: 2:03 - loss: 2.8355 - acc: 0.300 - ETA: 2:22 - loss: 2.7624 - acc: 0.350 - ETA: 2:22 - loss: 2.7523 - acc: 0.350 - ETA: 2:13 - loss: 2.7831 - acc: 0.312 - ETA: 2:13 - loss: 2.7805 - acc: 0.280 - ETA: 2:14 - loss: 2.7527 - acc: 0.291 - ETA: 2:08 - loss: 2.7740 - acc: 0.271 - ETA: 2:06 - loss: 2.7727 - acc: 0.275 - ETA: 2:05 - loss: 2.7770 - acc: 0.266 - ETA: 2:01 - loss: 2.7896 - acc: 0.260 - ETA: 2:02 - loss: 2.7842 - acc: 0.259 - ETA: 1:57 - loss: 2.7979 - acc: 0.254 - ETA: 1:53 - loss: 2.8065 - acc: 0.253 - ETA: 1:51 - loss: 2.8154 - acc: 0.246 - ETA: 1:50 - loss: 2.8149 - acc: 0.246 - ETA: 1:49 - loss: 2.8133 - acc: 0.243 - ETA: 1:46 - loss: 2.8044 - acc: 0.255 - ETA: 1:41 - loss: 2.8019 - acc: 0.258 - ETA: 1:39 - loss: 2.8036 - acc: 0.260 - ETA: 1:36 - loss: 2.7999 - acc: 0.260 - ETA: 1:32 - loss: 2.7878 - acc: 0.269 - ETA: 1:29 - loss: 2.7821 - acc: 0.270 - ETA: 1:27 - loss: 2.7839 - acc: 0.271 - ETA: 1:23 - loss: 2.7844 - acc: 0.270 - ETA: 1:20 - loss: 2.7861 - acc: 0.266 - ETA: 1:18 - loss: 2.7904 - acc: 0.261 - ETA: 1:15 - loss: 2.7894 - acc: 0.261 - ETA: 1:12 - loss: 2.7864 - acc: 0.260 - ETA: 1:09 - loss: 2.7908 - acc: 0.262 - ETA: 1:06 - loss: 2.7880 - acc: 0.260 - ETA: 1:03 - loss: 2.7888 - acc: 0.261 - ETA: 1:00 - loss: 2.7928 - acc: 0.254 - ETA: 57s - loss: 2.7927 - acc: 0.253 - ETA: 54s - loss: 2.7929 - acc: 0.25 - ETA: 51s - loss: 2.8024 - acc: 0.24 - ETA: 48s - loss: 2.8036 - acc: 0.24 - ETA: 44s - loss: 2.8032 - acc: 0.24 - ETA: 42s - loss: 2.8005 - acc: 0.25 - ETA: 39s - loss: 2.7996 - acc: 0.25 - ETA: 35s - loss: 2.7952 - acc: 0.25 - ETA: 32s - loss: 2.7933 - acc: 0.25 - ETA: 30s - loss: 2.7937 - acc: 0.25 - ETA: 27s - loss: 2.7979 - acc: 0.25 - ETA: 24s - loss: 2.7974 - acc: 0.25 - ETA: 21s - loss: 2.7973 - acc: 0.25 - ETA: 18s - loss: 2.7930 - acc: 0.25 - ETA: 15s - loss: 2.7939 - acc: 0.24 - ETA: 12s - loss: 2.7897 - acc: 0.25 - ETA: 9s - loss: 2.7912 - acc: 0.2490 - ETA: 6s - loss: 2.7907 - acc: 0.250 - ETA: 3s - loss: 2.7885 - acc: 0.2490Epoch 00009: val_loss improved from 3.10371 to 2.98415, saving model to saved_models/weights.best.benchmark_Model.hdf5\n",
      "1040/1040 [==============================] - 179s 173ms/step - loss: 2.7872 - acc: 0.2510 - val_loss: 2.9841 - val_acc: 0.1269\n",
      "Epoch 10/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1020/1040 [============================>.] - ETA: 2:19 - loss: 2.6967 - acc: 0.350 - ETA: 2:11 - loss: 2.7224 - acc: 0.250 - ETA: 2:17 - loss: 2.7956 - acc: 0.216 - ETA: 2:18 - loss: 2.7916 - acc: 0.200 - ETA: 2:11 - loss: 2.7964 - acc: 0.220 - ETA: 2:11 - loss: 2.7911 - acc: 0.225 - ETA: 2:10 - loss: 2.7659 - acc: 0.257 - ETA: 2:06 - loss: 2.7672 - acc: 0.237 - ETA: 2:08 - loss: 2.7670 - acc: 0.227 - ETA: 2:03 - loss: 2.7503 - acc: 0.245 - ETA: 2:04 - loss: 2.7441 - acc: 0.250 - ETA: 2:01 - loss: 2.7517 - acc: 0.250 - ETA: 1:58 - loss: 2.7369 - acc: 0.253 - ETA: 1:54 - loss: 2.7374 - acc: 0.257 - ETA: 1:51 - loss: 2.7235 - acc: 0.270 - ETA: 1:48 - loss: 2.7287 - acc: 0.268 - ETA: 1:44 - loss: 2.7295 - acc: 0.270 - ETA: 1:41 - loss: 2.7324 - acc: 0.275 - ETA: 1:39 - loss: 2.7239 - acc: 0.281 - ETA: 1:35 - loss: 2.7229 - acc: 0.285 - ETA: 1:32 - loss: 2.7265 - acc: 0.288 - ETA: 1:30 - loss: 2.7294 - acc: 0.290 - ETA: 1:27 - loss: 2.7355 - acc: 0.289 - ETA: 1:24 - loss: 2.7320 - acc: 0.289 - ETA: 1:21 - loss: 2.7327 - acc: 0.286 - ETA: 1:18 - loss: 2.7325 - acc: 0.286 - ETA: 1:15 - loss: 2.7352 - acc: 0.283 - ETA: 1:11 - loss: 2.7337 - acc: 0.282 - ETA: 1:08 - loss: 2.7359 - acc: 0.281 - ETA: 1:06 - loss: 2.7437 - acc: 0.278 - ETA: 1:02 - loss: 2.7465 - acc: 0.274 - ETA: 59s - loss: 2.7430 - acc: 0.278 - ETA: 57s - loss: 2.7413 - acc: 0.28 - ETA: 53s - loss: 2.7396 - acc: 0.28 - ETA: 50s - loss: 2.7324 - acc: 0.28 - ETA: 47s - loss: 2.7300 - acc: 0.29 - ETA: 44s - loss: 2.7274 - acc: 0.29 - ETA: 41s - loss: 2.7274 - acc: 0.29 - ETA: 38s - loss: 2.7260 - acc: 0.29 - ETA: 35s - loss: 2.7276 - acc: 0.29 - ETA: 32s - loss: 2.7295 - acc: 0.28 - ETA: 29s - loss: 2.7303 - acc: 0.28 - ETA: 26s - loss: 2.7336 - acc: 0.28 - ETA: 23s - loss: 2.7334 - acc: 0.28 - ETA: 20s - loss: 2.7361 - acc: 0.28 - ETA: 17s - loss: 2.7376 - acc: 0.28 - ETA: 14s - loss: 2.7426 - acc: 0.27 - ETA: 11s - loss: 2.7450 - acc: 0.27 - ETA: 8s - loss: 2.7445 - acc: 0.2755 - ETA: 5s - loss: 2.7497 - acc: 0.273 - ETA: 2s - loss: 2.7467 - acc: 0.2725Epoch 00010: val_loss improved from 2.98415 to 2.87577, saving model to saved_models/weights.best.benchmark_Model.hdf5\n",
      "1040/1040 [==============================] - 177s 170ms/step - loss: 2.7468 - acc: 0.2712 - val_loss: 2.8758 - val_acc: 0.1923\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint  \n",
    "\n",
    "\n",
    "epochs = 10\n",
    "\n",
    "\n",
    "checkpointer0 = ModelCheckpoint(filepath='saved_models/weights.best.benchmark_Model.hdf5', \n",
    "                               verbose=1, save_best_only=True)\n",
    "\n",
    "history0= model0.fit(train_tensors, train_targets, \n",
    "          validation_data=(valid_tensors, valid_targets),\n",
    "          epochs=epochs, batch_size=20, callbacks=[checkpointer0], verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the Model and Evaluate Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 19.2308%\n"
     ]
    }
   ],
   "source": [
    "# get index of predicted dog Iris for each image in test set\n",
    "iris_predictions = [np.argmax(model0.predict(np.expand_dims(tensor, axis=0))) for tensor in test_tensors]\n",
    "\n",
    "# report test accuracy\n",
    "test_accuracy = 100*np.sum(np.array(iris_predictions)==np.argmax(test_targets, axis=1))/len(iris_predictions)\n",
    "print('Test accuracy: %.4f%%' % test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Our Proposed Model\n",
    "\n",
    "Our proposed model consists of 5 CNV layers followed by 5 max pooling layers. Finally a global average pooling is used. Table 1 shows the model summary for each layer and its output shape. Images are imput in 224 × 224. Layers have number filters of 16, 32, 64, 128, 256 for layers 1, 2, 3, 4, 5 respectively. Finally a dense layer with size of 26 is used for classification decision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "batch_normalization_3 (Batch (None, 224, 224, 3)       12        \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 222, 222, 16)      448       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 111, 111, 16)      0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 111, 111, 16)      64        \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 109, 109, 32)      4640      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 54, 54, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 54, 54, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 52, 52, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 26, 26, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 26, 26, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 24, 24, 128)       73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 12, 12, 128)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 12, 12, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 10, 10, 256)       295168    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 5, 5, 256)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 5, 5, 256)         1024      \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_2 ( (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 26)                6682      \n",
      "=================================================================\n",
      "Total params: 401,286\n",
      "Trainable params: 400,288\n",
      "Non-trainable params: 998\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D\n",
    "from keras.layers import Dropout, Flatten, Dense, Activation\n",
    "from keras.models import Sequential\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(BatchNormalization(input_shape=(224, 224, 3)))\n",
    "model.add(Conv2D(filters=16, kernel_size=3, kernel_initializer='he_normal', activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Conv2D(filters=32, kernel_size=3, kernel_initializer='he_normal', activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Conv2D(filters=64, kernel_size=3, kernel_initializer='he_normal', activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Conv2D(filters=128, kernel_size=3, kernel_initializer='he_normal', activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Conv2D(filters=256, kernel_size=3, kernel_initializer='he_normal', activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(GlobalAveragePooling2D())\n",
    "\n",
    "model.add(Dense(26, activation='softmax'))\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1040 samples, validate on 260 samples\n",
      "Epoch 1/10\n",
      "1020/1040 [============================>.] - ETA: 5:27 - loss: 3.2427 - acc: 0.0000e+0 - ETA: 3:48 - loss: 3.1304 - acc: 0.0750    - ETA: 3:36 - loss: 2.9482 - acc: 0.183 - ETA: 3:12 - loss: 2.7952 - acc: 0.250 - ETA: 3:09 - loss: 2.8278 - acc: 0.230 - ETA: 2:57 - loss: 2.7513 - acc: 0.258 - ETA: 2:57 - loss: 2.7205 - acc: 0.285 - ETA: 2:57 - loss: 2.6759 - acc: 0.293 - ETA: 2:47 - loss: 2.6288 - acc: 0.322 - ETA: 2:44 - loss: 2.5649 - acc: 0.340 - ETA: 2:36 - loss: 2.5234 - acc: 0.354 - ETA: 2:33 - loss: 2.4775 - acc: 0.370 - ETA: 2:30 - loss: 2.4257 - acc: 0.384 - ETA: 2:23 - loss: 2.3934 - acc: 0.400 - ETA: 2:21 - loss: 2.3453 - acc: 0.420 - ETA: 2:16 - loss: 2.2853 - acc: 0.453 - ETA: 2:12 - loss: 2.2471 - acc: 0.464 - ETA: 2:09 - loss: 2.2131 - acc: 0.472 - ETA: 2:05 - loss: 2.1942 - acc: 0.473 - ETA: 2:00 - loss: 2.1636 - acc: 0.485 - ETA: 1:57 - loss: 2.1298 - acc: 0.497 - ETA: 1:52 - loss: 2.0908 - acc: 0.511 - ETA: 1:49 - loss: 2.0592 - acc: 0.523 - ETA: 1:44 - loss: 2.0321 - acc: 0.535 - ETA: 1:40 - loss: 2.0055 - acc: 0.546 - ETA: 1:36 - loss: 1.9665 - acc: 0.557 - ETA: 1:32 - loss: 1.9458 - acc: 0.568 - ETA: 1:29 - loss: 1.9262 - acc: 0.573 - ETA: 1:25 - loss: 1.8971 - acc: 0.579 - ETA: 1:21 - loss: 1.8737 - acc: 0.588 - ETA: 1:17 - loss: 1.8461 - acc: 0.596 - ETA: 1:14 - loss: 1.8224 - acc: 0.604 - ETA: 1:10 - loss: 1.7956 - acc: 0.612 - ETA: 1:06 - loss: 1.7638 - acc: 0.622 - ETA: 1:02 - loss: 1.7372 - acc: 0.630 - ETA: 58s - loss: 1.7136 - acc: 0.637 - ETA: 55s - loss: 1.6911 - acc: 0.64 - ETA: 51s - loss: 1.6658 - acc: 0.65 - ETA: 47s - loss: 1.6419 - acc: 0.65 - ETA: 43s - loss: 1.6203 - acc: 0.66 - ETA: 40s - loss: 1.6036 - acc: 0.67 - ETA: 36s - loss: 1.5845 - acc: 0.67 - ETA: 32s - loss: 1.5648 - acc: 0.68 - ETA: 29s - loss: 1.5472 - acc: 0.68 - ETA: 25s - loss: 1.5289 - acc: 0.69 - ETA: 22s - loss: 1.5127 - acc: 0.69 - ETA: 18s - loss: 1.4958 - acc: 0.70 - ETA: 14s - loss: 1.4780 - acc: 0.70 - ETA: 11s - loss: 1.4612 - acc: 0.70 - ETA: 7s - loss: 1.4422 - acc: 0.7120 - ETA: 3s - loss: 1.4229 - acc: 0.7167Epoch 00001: val_loss improved from inf to 3.89507, saving model to saved_models/weights.best.iris_Model.hdf5\n",
      "1040/1040 [==============================] - 224s 216ms/step - loss: 1.4071 - acc: 0.7202 - val_loss: 3.8951 - val_acc: 0.0423\n",
      "Epoch 2/10\n",
      "1020/1040 [============================>.] - ETA: 2:33 - loss: 0.2754 - acc: 1.000 - ETA: 3:04 - loss: 0.3651 - acc: 0.975 - ETA: 3:12 - loss: 0.4961 - acc: 0.900 - ETA: 2:58 - loss: 0.5085 - acc: 0.912 - ETA: 2:59 - loss: 0.4923 - acc: 0.930 - ETA: 3:01 - loss: 0.4940 - acc: 0.933 - ETA: 3:00 - loss: 0.4855 - acc: 0.942 - ETA: 2:50 - loss: 0.4632 - acc: 0.950 - ETA: 2:49 - loss: 0.4623 - acc: 0.950 - ETA: 2:47 - loss: 0.4545 - acc: 0.955 - ETA: 2:43 - loss: 0.4780 - acc: 0.954 - ETA: 2:37 - loss: 0.4735 - acc: 0.954 - ETA: 2:34 - loss: 0.4587 - acc: 0.957 - ETA: 2:31 - loss: 0.4453 - acc: 0.960 - ETA: 2:26 - loss: 0.4391 - acc: 0.963 - ETA: 2:23 - loss: 0.4299 - acc: 0.962 - ETA: 2:20 - loss: 0.4213 - acc: 0.964 - ETA: 2:14 - loss: 0.4159 - acc: 0.963 - ETA: 2:11 - loss: 0.4133 - acc: 0.963 - ETA: 2:08 - loss: 0.4117 - acc: 0.965 - ETA: 2:04 - loss: 0.4071 - acc: 0.966 - ETA: 1:59 - loss: 0.3979 - acc: 0.968 - ETA: 1:56 - loss: 0.3923 - acc: 0.969 - ETA: 1:52 - loss: 0.3866 - acc: 0.970 - ETA: 1:49 - loss: 0.3838 - acc: 0.970 - ETA: 1:44 - loss: 0.3775 - acc: 0.971 - ETA: 1:40 - loss: 0.3753 - acc: 0.972 - ETA: 1:37 - loss: 0.3702 - acc: 0.973 - ETA: 1:32 - loss: 0.3643 - acc: 0.974 - ETA: 1:28 - loss: 0.3621 - acc: 0.973 - ETA: 1:24 - loss: 0.3605 - acc: 0.974 - ETA: 1:20 - loss: 0.3628 - acc: 0.971 - ETA: 1:16 - loss: 0.3570 - acc: 0.972 - ETA: 1:12 - loss: 0.3544 - acc: 0.972 - ETA: 1:08 - loss: 0.3487 - acc: 0.972 - ETA: 1:04 - loss: 0.3457 - acc: 0.973 - ETA: 1:00 - loss: 0.3408 - acc: 0.974 - ETA: 56s - loss: 0.3372 - acc: 0.975 - ETA: 52s - loss: 0.3326 - acc: 0.97 - ETA: 48s - loss: 0.3289 - acc: 0.97 - ETA: 44s - loss: 0.3277 - acc: 0.97 - ETA: 40s - loss: 0.3295 - acc: 0.97 - ETA: 36s - loss: 0.3285 - acc: 0.97 - ETA: 32s - loss: 0.3277 - acc: 0.97 - ETA: 28s - loss: 0.3259 - acc: 0.97 - ETA: 24s - loss: 0.3228 - acc: 0.97 - ETA: 20s - loss: 0.3211 - acc: 0.97 - ETA: 16s - loss: 0.3217 - acc: 0.97 - ETA: 12s - loss: 0.3195 - acc: 0.97 - ETA: 8s - loss: 0.3166 - acc: 0.9720 - ETA: 4s - loss: 0.3122 - acc: 0.9725Epoch 00002: val_loss did not improve\n",
      "1040/1040 [==============================] - 235s 226ms/step - loss: 0.3088 - acc: 0.9731 - val_loss: 6.4461 - val_acc: 0.0385\n",
      "Epoch 3/10\n",
      "1020/1040 [============================>.] - ETA: 3:59 - loss: 0.1211 - acc: 1.000 - ETA: 3:47 - loss: 0.0842 - acc: 1.000 - ETA: 3:24 - loss: 0.1106 - acc: 0.983 - ETA: 3:22 - loss: 0.1025 - acc: 0.987 - ETA: 3:23 - loss: 0.0943 - acc: 0.990 - ETA: 3:14 - loss: 0.0913 - acc: 0.991 - ETA: 3:05 - loss: 0.0932 - acc: 0.992 - ETA: 3:02 - loss: 0.0923 - acc: 0.993 - ETA: 2:53 - loss: 0.0950 - acc: 0.994 - ETA: 2:51 - loss: 0.1000 - acc: 0.995 - ETA: 2:49 - loss: 0.1137 - acc: 0.990 - ETA: 2:47 - loss: 0.1119 - acc: 0.991 - ETA: 2:40 - loss: 0.1103 - acc: 0.992 - ETA: 2:36 - loss: 0.1136 - acc: 0.992 - ETA: 2:33 - loss: 0.1178 - acc: 0.990 - ETA: 2:29 - loss: 0.1253 - acc: 0.990 - ETA: 2:24 - loss: 0.1287 - acc: 0.988 - ETA: 2:20 - loss: 0.1289 - acc: 0.988 - ETA: 2:14 - loss: 0.1279 - acc: 0.989 - ETA: 2:10 - loss: 0.1256 - acc: 0.990 - ETA: 2:07 - loss: 0.1259 - acc: 0.988 - ETA: 2:01 - loss: 0.1262 - acc: 0.988 - ETA: 1:57 - loss: 0.1235 - acc: 0.989 - ETA: 1:54 - loss: 0.1211 - acc: 0.989 - ETA: 1:49 - loss: 0.1175 - acc: 0.990 - ETA: 1:45 - loss: 0.1213 - acc: 0.988 - ETA: 1:41 - loss: 0.1212 - acc: 0.988 - ETA: 1:36 - loss: 0.1194 - acc: 0.989 - ETA: 1:32 - loss: 0.1170 - acc: 0.989 - ETA: 1:28 - loss: 0.1188 - acc: 0.990 - ETA: 1:24 - loss: 0.1212 - acc: 0.990 - ETA: 1:20 - loss: 0.1230 - acc: 0.989 - ETA: 1:16 - loss: 0.1244 - acc: 0.987 - ETA: 1:12 - loss: 0.1273 - acc: 0.986 - ETA: 1:08 - loss: 0.1306 - acc: 0.985 - ETA: 1:04 - loss: 0.1353 - acc: 0.984 - ETA: 59s - loss: 0.1371 - acc: 0.982 - ETA: 55s - loss: 0.1385 - acc: 0.98 - ETA: 51s - loss: 0.1374 - acc: 0.98 - ETA: 47s - loss: 0.1385 - acc: 0.98 - ETA: 43s - loss: 0.1384 - acc: 0.98 - ETA: 39s - loss: 0.1370 - acc: 0.98 - ETA: 35s - loss: 0.1351 - acc: 0.98 - ETA: 31s - loss: 0.1328 - acc: 0.98 - ETA: 27s - loss: 0.1329 - acc: 0.98 - ETA: 23s - loss: 0.1315 - acc: 0.98 - ETA: 19s - loss: 0.1294 - acc: 0.98 - ETA: 15s - loss: 0.1290 - acc: 0.98 - ETA: 11s - loss: 0.1296 - acc: 0.98 - ETA: 7s - loss: 0.1294 - acc: 0.9840 - ETA: 3s - loss: 0.1284 - acc: 0.9843Epoch 00003: val_loss did not improve\n",
      "1040/1040 [==============================] - 233s 224ms/step - loss: 0.1272 - acc: 0.9846 - val_loss: 10.2225 - val_acc: 0.0769\n",
      "Epoch 4/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1020/1040 [============================>.] - ETA: 3:07 - loss: 0.0511 - acc: 1.000 - ETA: 3:26 - loss: 0.0495 - acc: 1.000 - ETA: 3:16 - loss: 0.0409 - acc: 1.000 - ETA: 3:08 - loss: 0.0366 - acc: 1.000 - ETA: 3:10 - loss: 0.0388 - acc: 1.000 - ETA: 3:02 - loss: 0.0382 - acc: 1.000 - ETA: 2:58 - loss: 0.0358 - acc: 1.000 - ETA: 2:57 - loss: 0.0333 - acc: 1.000 - ETA: 2:50 - loss: 0.0320 - acc: 1.000 - ETA: 2:47 - loss: 0.0308 - acc: 1.000 - ETA: 2:45 - loss: 0.0321 - acc: 1.000 - ETA: 2:42 - loss: 0.0328 - acc: 1.000 - ETA: 2:37 - loss: 0.0338 - acc: 1.000 - ETA: 2:33 - loss: 0.0344 - acc: 1.000 - ETA: 2:30 - loss: 0.0365 - acc: 1.000 - ETA: 2:25 - loss: 0.0371 - acc: 1.000 - ETA: 2:21 - loss: 0.0384 - acc: 1.000 - ETA: 2:18 - loss: 0.0397 - acc: 1.000 - ETA: 2:13 - loss: 0.0401 - acc: 1.000 - ETA: 2:09 - loss: 0.0410 - acc: 1.000 - ETA: 2:06 - loss: 0.0455 - acc: 0.997 - ETA: 2:02 - loss: 0.0533 - acc: 0.995 - ETA: 1:58 - loss: 0.0531 - acc: 0.995 - ETA: 1:53 - loss: 0.0531 - acc: 0.995 - ETA: 1:50 - loss: 0.0525 - acc: 0.996 - ETA: 1:45 - loss: 0.0522 - acc: 0.996 - ETA: 1:41 - loss: 0.0547 - acc: 0.994 - ETA: 1:37 - loss: 0.0604 - acc: 0.992 - ETA: 1:33 - loss: 0.0611 - acc: 0.993 - ETA: 1:29 - loss: 0.0612 - acc: 0.993 - ETA: 1:25 - loss: 0.0611 - acc: 0.993 - ETA: 1:21 - loss: 0.0613 - acc: 0.993 - ETA: 1:16 - loss: 0.0601 - acc: 0.993 - ETA: 1:12 - loss: 0.0588 - acc: 0.994 - ETA: 1:08 - loss: 0.0581 - acc: 0.994 - ETA: 1:04 - loss: 0.0572 - acc: 0.994 - ETA: 1:00 - loss: 0.0570 - acc: 0.994 - ETA: 56s - loss: 0.0564 - acc: 0.994 - ETA: 52s - loss: 0.0555 - acc: 0.99 - ETA: 48s - loss: 0.0550 - acc: 0.99 - ETA: 44s - loss: 0.0541 - acc: 0.99 - ETA: 40s - loss: 0.0531 - acc: 0.99 - ETA: 36s - loss: 0.0524 - acc: 0.99 - ETA: 32s - loss: 0.0515 - acc: 0.99 - ETA: 28s - loss: 0.0506 - acc: 0.99 - ETA: 24s - loss: 0.0498 - acc: 0.99 - ETA: 20s - loss: 0.0494 - acc: 0.99 - ETA: 16s - loss: 0.0488 - acc: 0.99 - ETA: 12s - loss: 0.0480 - acc: 0.99 - ETA: 8s - loss: 0.0485 - acc: 0.9950 - ETA: 3s - loss: 0.0518 - acc: 0.9941Epoch 00004: val_loss did not improve\n",
      "1040/1040 [==============================] - 235s 226ms/step - loss: 0.0536 - acc: 0.9942 - val_loss: 8.1851 - val_acc: 0.0731\n",
      "Epoch 5/10\n",
      "1020/1040 [============================>.] - ETA: 3:45 - loss: 0.0608 - acc: 1.000 - ETA: 3:20 - loss: 0.0466 - acc: 1.000 - ETA: 3:15 - loss: 0.0369 - acc: 1.000 - ETA: 3:18 - loss: 0.0302 - acc: 1.000 - ETA: 3:08 - loss: 0.0296 - acc: 1.000 - ETA: 3:04 - loss: 0.0320 - acc: 1.000 - ETA: 3:03 - loss: 0.0293 - acc: 1.000 - ETA: 2:55 - loss: 0.0277 - acc: 1.000 - ETA: 2:52 - loss: 0.0272 - acc: 1.000 - ETA: 2:49 - loss: 0.0260 - acc: 1.000 - ETA: 2:41 - loss: 0.0244 - acc: 1.000 - ETA: 2:39 - loss: 0.0240 - acc: 1.000 - ETA: 2:37 - loss: 0.0242 - acc: 1.000 - ETA: 2:30 - loss: 0.0235 - acc: 1.000 - ETA: 2:27 - loss: 0.0225 - acc: 1.000 - ETA: 2:21 - loss: 0.0221 - acc: 1.000 - ETA: 2:18 - loss: 0.0212 - acc: 1.000 - ETA: 2:15 - loss: 0.0207 - acc: 1.000 - ETA: 2:09 - loss: 0.0205 - acc: 1.000 - ETA: 2:06 - loss: 0.0199 - acc: 1.000 - ETA: 2:03 - loss: 0.0192 - acc: 1.000 - ETA: 1:57 - loss: 0.0186 - acc: 1.000 - ETA: 1:54 - loss: 0.0181 - acc: 1.000 - ETA: 1:49 - loss: 0.0178 - acc: 1.000 - ETA: 1:45 - loss: 0.0186 - acc: 1.000 - ETA: 1:42 - loss: 0.0197 - acc: 1.000 - ETA: 1:37 - loss: 0.0225 - acc: 1.000 - ETA: 1:34 - loss: 0.0276 - acc: 0.998 - ETA: 1:30 - loss: 0.0296 - acc: 0.998 - ETA: 1:26 - loss: 0.0291 - acc: 0.998 - ETA: 1:22 - loss: 0.0289 - acc: 0.998 - ETA: 1:18 - loss: 0.0292 - acc: 0.998 - ETA: 1:14 - loss: 0.0289 - acc: 0.998 - ETA: 1:10 - loss: 0.0288 - acc: 0.998 - ETA: 1:06 - loss: 0.0284 - acc: 0.998 - ETA: 1:02 - loss: 0.0280 - acc: 0.998 - ETA: 58s - loss: 0.0284 - acc: 0.998 - ETA: 55s - loss: 0.0279 - acc: 0.99 - ETA: 51s - loss: 0.0275 - acc: 0.99 - ETA: 47s - loss: 0.0271 - acc: 0.99 - ETA: 43s - loss: 0.0268 - acc: 0.99 - ETA: 39s - loss: 0.0264 - acc: 0.99 - ETA: 35s - loss: 0.0262 - acc: 0.99 - ETA: 31s - loss: 0.0258 - acc: 0.99 - ETA: 27s - loss: 0.0256 - acc: 0.99 - ETA: 23s - loss: 0.0252 - acc: 0.99 - ETA: 19s - loss: 0.0249 - acc: 0.99 - ETA: 15s - loss: 0.0247 - acc: 0.99 - ETA: 11s - loss: 0.0243 - acc: 0.99 - ETA: 7s - loss: 0.0240 - acc: 0.9990 - ETA: 3s - loss: 0.0236 - acc: 0.9990Epoch 00005: val_loss did not improve\n",
      "1040/1040 [==============================] - 230s 221ms/step - loss: 0.0233 - acc: 0.9990 - val_loss: 10.2248 - val_acc: 0.0769\n",
      "Epoch 6/10\n",
      "1020/1040 [============================>.] - ETA: 3:49 - loss: 0.0083 - acc: 1.000 - ETA: 3:41 - loss: 0.0089 - acc: 1.000 - ETA: 3:12 - loss: 0.0091 - acc: 1.000 - ETA: 3:13 - loss: 0.0131 - acc: 1.000 - ETA: 3:04 - loss: 0.0171 - acc: 1.000 - ETA: 3:00 - loss: 0.0189 - acc: 1.000 - ETA: 2:59 - loss: 0.0245 - acc: 1.000 - ETA: 2:50 - loss: 0.0285 - acc: 1.000 - ETA: 2:49 - loss: 0.0292 - acc: 1.000 - ETA: 2:47 - loss: 0.0281 - acc: 1.000 - ETA: 2:45 - loss: 0.0260 - acc: 1.000 - ETA: 2:39 - loss: 0.0248 - acc: 1.000 - ETA: 2:35 - loss: 0.0250 - acc: 1.000 - ETA: 2:33 - loss: 0.0249 - acc: 1.000 - ETA: 2:26 - loss: 0.0237 - acc: 1.000 - ETA: 2:23 - loss: 0.0224 - acc: 1.000 - ETA: 2:20 - loss: 0.0213 - acc: 1.000 - ETA: 2:16 - loss: 0.0204 - acc: 1.000 - ETA: 2:11 - loss: 0.0196 - acc: 1.000 - ETA: 2:08 - loss: 0.0189 - acc: 1.000 - ETA: 2:03 - loss: 0.0182 - acc: 1.000 - ETA: 1:59 - loss: 0.0177 - acc: 1.000 - ETA: 1:56 - loss: 0.0173 - acc: 1.000 - ETA: 1:50 - loss: 0.0171 - acc: 1.000 - ETA: 1:47 - loss: 0.0167 - acc: 1.000 - ETA: 1:43 - loss: 0.0164 - acc: 1.000 - ETA: 1:38 - loss: 0.0160 - acc: 1.000 - ETA: 1:35 - loss: 0.0158 - acc: 1.000 - ETA: 1:30 - loss: 0.0155 - acc: 1.000 - ETA: 1:26 - loss: 0.0153 - acc: 1.000 - ETA: 1:23 - loss: 0.0152 - acc: 1.000 - ETA: 1:19 - loss: 0.0151 - acc: 1.000 - ETA: 1:14 - loss: 0.0151 - acc: 1.000 - ETA: 1:11 - loss: 0.0154 - acc: 1.000 - ETA: 1:07 - loss: 0.0221 - acc: 0.998 - ETA: 1:03 - loss: 0.0225 - acc: 0.998 - ETA: 59s - loss: 0.0233 - acc: 0.998 - ETA: 55s - loss: 0.0235 - acc: 0.99 - ETA: 51s - loss: 0.0237 - acc: 0.99 - ETA: 47s - loss: 0.0236 - acc: 0.99 - ETA: 43s - loss: 0.0232 - acc: 0.99 - ETA: 39s - loss: 0.0228 - acc: 0.99 - ETA: 35s - loss: 0.0227 - acc: 0.99 - ETA: 31s - loss: 0.0223 - acc: 0.99 - ETA: 27s - loss: 0.0219 - acc: 0.99 - ETA: 23s - loss: 0.0220 - acc: 0.99 - ETA: 19s - loss: 0.0217 - acc: 0.99 - ETA: 15s - loss: 0.0213 - acc: 0.99 - ETA: 11s - loss: 0.0212 - acc: 0.99 - ETA: 7s - loss: 0.0209 - acc: 0.9990 - ETA: 3s - loss: 0.0206 - acc: 0.9990Epoch 00006: val_loss did not improve\n",
      "1040/1040 [==============================] - 230s 221ms/step - loss: 0.0206 - acc: 0.9990 - val_loss: 9.6548 - val_acc: 0.0692\n",
      "Epoch 7/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1020/1040 [============================>.] - ETA: 3:44 - loss: 0.3278 - acc: 0.850 - ETA: 3:20 - loss: 0.2083 - acc: 0.925 - ETA: 3:12 - loss: 0.1455 - acc: 0.950 - ETA: 3:12 - loss: 0.1315 - acc: 0.950 - ETA: 2:59 - loss: 0.1090 - acc: 0.960 - ETA: 3:00 - loss: 0.0938 - acc: 0.966 - ETA: 2:59 - loss: 0.0825 - acc: 0.971 - ETA: 2:49 - loss: 0.0732 - acc: 0.975 - ETA: 2:50 - loss: 0.0658 - acc: 0.977 - ETA: 2:47 - loss: 0.0599 - acc: 0.980 - ETA: 2:39 - loss: 0.0552 - acc: 0.981 - ETA: 2:37 - loss: 0.0514 - acc: 0.983 - ETA: 2:30 - loss: 0.0478 - acc: 0.984 - ETA: 2:28 - loss: 0.0448 - acc: 0.985 - ETA: 2:25 - loss: 0.0422 - acc: 0.986 - ETA: 2:19 - loss: 0.0397 - acc: 0.987 - ETA: 2:17 - loss: 0.0375 - acc: 0.988 - ETA: 2:14 - loss: 0.0358 - acc: 0.988 - ETA: 2:11 - loss: 0.0341 - acc: 0.989 - ETA: 2:05 - loss: 0.0325 - acc: 0.990 - ETA: 2:02 - loss: 0.0312 - acc: 0.990 - ETA: 1:59 - loss: 0.0299 - acc: 0.990 - ETA: 1:54 - loss: 0.0286 - acc: 0.991 - ETA: 1:50 - loss: 0.0275 - acc: 0.991 - ETA: 1:46 - loss: 0.0265 - acc: 0.992 - ETA: 1:42 - loss: 0.0255 - acc: 0.992 - ETA: 1:38 - loss: 0.0248 - acc: 0.992 - ETA: 1:33 - loss: 0.0239 - acc: 0.992 - ETA: 1:29 - loss: 0.0232 - acc: 0.993 - ETA: 1:25 - loss: 0.0225 - acc: 0.993 - ETA: 1:21 - loss: 0.0219 - acc: 0.993 - ETA: 1:17 - loss: 0.0213 - acc: 0.993 - ETA: 1:13 - loss: 0.0208 - acc: 0.993 - ETA: 1:09 - loss: 0.0205 - acc: 0.994 - ETA: 1:05 - loss: 0.0201 - acc: 0.994 - ETA: 1:01 - loss: 0.0197 - acc: 0.994 - ETA: 57s - loss: 0.0192 - acc: 0.994 - ETA: 53s - loss: 0.0187 - acc: 0.99 - ETA: 50s - loss: 0.0183 - acc: 0.99 - ETA: 46s - loss: 0.0179 - acc: 0.99 - ETA: 42s - loss: 0.0175 - acc: 0.99 - ETA: 38s - loss: 0.0171 - acc: 0.99 - ETA: 34s - loss: 0.0168 - acc: 0.99 - ETA: 30s - loss: 0.0164 - acc: 0.99 - ETA: 26s - loss: 0.0161 - acc: 0.99 - ETA: 23s - loss: 0.0158 - acc: 0.99 - ETA: 19s - loss: 0.0155 - acc: 0.99 - ETA: 15s - loss: 0.0152 - acc: 0.99 - ETA: 11s - loss: 0.0150 - acc: 0.99 - ETA: 7s - loss: 0.0147 - acc: 0.9960 - ETA: 3s - loss: 0.0146 - acc: 0.9961Epoch 00007: val_loss improved from 3.89507 to 1.97366, saving model to saved_models/weights.best.iris_Model.hdf5\n",
      "1040/1040 [==============================] - 225s 216ms/step - loss: 0.0145 - acc: 0.9962 - val_loss: 1.9737 - val_acc: 0.4654\n",
      "Epoch 8/10\n",
      "1020/1040 [============================>.] - ETA: 2:24 - loss: 0.0241 - acc: 1.000 - ETA: 2:47 - loss: 0.0151 - acc: 1.000 - ETA: 2:41 - loss: 0.0137 - acc: 1.000 - ETA: 2:48 - loss: 0.0113 - acc: 1.000 - ETA: 2:38 - loss: 0.0100 - acc: 1.000 - ETA: 2:41 - loss: 0.0098 - acc: 1.000 - ETA: 2:35 - loss: 0.0090 - acc: 1.000 - ETA: 2:35 - loss: 0.0272 - acc: 0.993 - ETA: 2:36 - loss: 0.0398 - acc: 0.988 - ETA: 2:29 - loss: 0.0375 - acc: 0.990 - ETA: 2:27 - loss: 0.0352 - acc: 0.990 - ETA: 2:21 - loss: 0.0330 - acc: 0.991 - ETA: 2:20 - loss: 0.0307 - acc: 0.992 - ETA: 2:17 - loss: 0.0292 - acc: 0.992 - ETA: 2:13 - loss: 0.0275 - acc: 0.993 - ETA: 2:10 - loss: 0.0276 - acc: 0.993 - ETA: 2:05 - loss: 0.0263 - acc: 0.994 - ETA: 2:03 - loss: 0.0276 - acc: 0.991 - ETA: 1:58 - loss: 0.0268 - acc: 0.992 - ETA: 1:55 - loss: 0.0257 - acc: 0.992 - ETA: 1:53 - loss: 0.0247 - acc: 0.992 - ETA: 1:48 - loss: 0.0238 - acc: 0.993 - ETA: 1:45 - loss: 0.0228 - acc: 0.993 - ETA: 1:42 - loss: 0.0222 - acc: 0.993 - ETA: 1:39 - loss: 0.0214 - acc: 0.994 - ETA: 1:36 - loss: 0.0207 - acc: 0.994 - ETA: 1:33 - loss: 0.0200 - acc: 0.994 - ETA: 1:29 - loss: 0.0194 - acc: 0.994 - ETA: 1:25 - loss: 0.0188 - acc: 0.994 - ETA: 1:22 - loss: 0.0182 - acc: 0.995 - ETA: 1:17 - loss: 0.0177 - acc: 0.995 - ETA: 1:14 - loss: 0.0173 - acc: 0.995 - ETA: 1:10 - loss: 0.0168 - acc: 0.995 - ETA: 1:06 - loss: 0.0164 - acc: 0.995 - ETA: 1:03 - loss: 0.0159 - acc: 0.995 - ETA: 59s - loss: 0.0155 - acc: 0.995 - ETA: 55s - loss: 0.0152 - acc: 0.99 - ETA: 51s - loss: 0.0149 - acc: 0.99 - ETA: 48s - loss: 0.0145 - acc: 0.99 - ETA: 44s - loss: 0.0142 - acc: 0.99 - ETA: 40s - loss: 0.0139 - acc: 0.99 - ETA: 37s - loss: 0.0136 - acc: 0.99 - ETA: 33s - loss: 0.0133 - acc: 0.99 - ETA: 29s - loss: 0.0131 - acc: 0.99 - ETA: 26s - loss: 0.0128 - acc: 0.99 - ETA: 22s - loss: 0.0125 - acc: 0.99 - ETA: 18s - loss: 0.0123 - acc: 0.99 - ETA: 15s - loss: 0.0120 - acc: 0.99 - ETA: 11s - loss: 0.0118 - acc: 0.99 - ETA: 7s - loss: 0.0116 - acc: 0.9970 - ETA: 3s - loss: 0.0114 - acc: 0.9971Epoch 00008: val_loss improved from 1.97366 to 0.14624, saving model to saved_models/weights.best.iris_Model.hdf5\n",
      "1040/1040 [==============================] - 222s 214ms/step - loss: 0.0112 - acc: 0.9971 - val_loss: 0.1462 - val_acc: 0.9692\n",
      "Epoch 9/10\n",
      "1020/1040 [============================>.] - ETA: 3:05 - loss: 0.0024 - acc: 1.000 - ETA: 3:21 - loss: 0.0052 - acc: 1.000 - ETA: 3:23 - loss: 0.0073 - acc: 1.000 - ETA: 3:05 - loss: 0.0083 - acc: 1.000 - ETA: 3:05 - loss: 0.0079 - acc: 1.000 - ETA: 2:57 - loss: 0.0491 - acc: 0.983 - ETA: 2:53 - loss: 0.0576 - acc: 0.985 - ETA: 2:52 - loss: 0.0549 - acc: 0.987 - ETA: 2:43 - loss: 0.0550 - acc: 0.983 - ETA: 2:42 - loss: 0.0535 - acc: 0.985 - ETA: 2:40 - loss: 0.0490 - acc: 0.986 - ETA: 2:34 - loss: 0.0460 - acc: 0.987 - ETA: 2:31 - loss: 0.0438 - acc: 0.988 - ETA: 2:26 - loss: 0.0411 - acc: 0.989 - ETA: 2:23 - loss: 0.0388 - acc: 0.990 - ETA: 2:20 - loss: 0.0367 - acc: 0.990 - ETA: 2:14 - loss: 0.0347 - acc: 0.991 - ETA: 2:11 - loss: 0.0329 - acc: 0.991 - ETA: 2:06 - loss: 0.0313 - acc: 0.992 - ETA: 2:02 - loss: 0.0301 - acc: 0.992 - ETA: 2:00 - loss: 0.0288 - acc: 0.992 - ETA: 1:56 - loss: 0.0277 - acc: 0.993 - ETA: 1:52 - loss: 0.0267 - acc: 0.993 - ETA: 1:48 - loss: 0.0257 - acc: 0.993 - ETA: 1:45 - loss: 0.0248 - acc: 0.994 - ETA: 1:40 - loss: 0.0239 - acc: 0.994 - ETA: 1:37 - loss: 0.0232 - acc: 0.994 - ETA: 1:33 - loss: 0.0225 - acc: 0.994 - ETA: 1:29 - loss: 0.0218 - acc: 0.994 - ETA: 1:25 - loss: 0.0214 - acc: 0.995 - ETA: 1:21 - loss: 0.0209 - acc: 0.995 - ETA: 1:17 - loss: 0.0203 - acc: 0.995 - ETA: 1:13 - loss: 0.0197 - acc: 0.995 - ETA: 1:09 - loss: 0.0191 - acc: 0.995 - ETA: 1:05 - loss: 0.0187 - acc: 0.995 - ETA: 1:02 - loss: 0.0182 - acc: 0.995 - ETA: 58s - loss: 0.0178 - acc: 0.995 - ETA: 54s - loss: 0.0174 - acc: 0.99 - ETA: 50s - loss: 0.0169 - acc: 0.99 - ETA: 46s - loss: 0.0165 - acc: 0.99 - ETA: 42s - loss: 0.0162 - acc: 0.99 - ETA: 38s - loss: 0.0158 - acc: 0.99 - ETA: 34s - loss: 0.0155 - acc: 0.99 - ETA: 31s - loss: 0.0151 - acc: 0.99 - ETA: 27s - loss: 0.0148 - acc: 0.99 - ETA: 23s - loss: 0.0145 - acc: 0.99 - ETA: 19s - loss: 0.0142 - acc: 0.99 - ETA: 15s - loss: 0.0140 - acc: 0.99 - ETA: 11s - loss: 0.0137 - acc: 0.99 - ETA: 7s - loss: 0.0134 - acc: 0.9970 - ETA: 3s - loss: 0.0132 - acc: 0.9971Epoch 00009: val_loss improved from 0.14624 to 0.00688, saving model to saved_models/weights.best.iris_Model.hdf5\n",
      "1040/1040 [==============================] - 230s 222ms/step - loss: 0.0130 - acc: 0.9971 - val_loss: 0.0069 - val_acc: 1.0000\n",
      "Epoch 10/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1020/1040 [============================>.] - ETA: 2:55 - loss: 9.3380e-04 - acc: 1.000 - ETA: 3:10 - loss: 6.3148e-04 - acc: 1.000 - ETA: 2:49 - loss: 5.9809e-04 - acc: 1.000 - ETA: 2:56 - loss: 6.3205e-04 - acc: 1.000 - ETA: 2:44 - loss: 5.9439e-04 - acc: 1.000 - ETA: 2:54 - loss: 5.4327e-04 - acc: 1.000 - ETA: 2:47 - loss: 5.8802e-04 - acc: 1.000 - ETA: 2:45 - loss: 5.6505e-04 - acc: 1.000 - ETA: 2:45 - loss: 5.6265e-04 - acc: 1.000 - ETA: 2:37 - loss: 5.3774e-04 - acc: 1.000 - ETA: 2:37 - loss: 5.1713e-04 - acc: 1.000 - ETA: 2:36 - loss: 4.9026e-04 - acc: 1.000 - ETA: 2:31 - loss: 4.7028e-04 - acc: 1.000 - ETA: 2:27 - loss: 4.6610e-04 - acc: 1.000 - ETA: 2:25 - loss: 5.3958e-04 - acc: 1.000 - ETA: 2:23 - loss: 5.3464e-04 - acc: 1.000 - ETA: 2:19 - loss: 5.2434e-04 - acc: 1.000 - ETA: 2:14 - loss: 5.2056e-04 - acc: 1.000 - ETA: 2:12 - loss: 5.0806e-04 - acc: 1.000 - ETA: 2:08 - loss: 4.9155e-04 - acc: 1.000 - ETA: 2:03 - loss: 5.0623e-04 - acc: 1.000 - ETA: 1:59 - loss: 5.3997e-04 - acc: 1.000 - ETA: 1:56 - loss: 5.3034e-04 - acc: 1.000 - ETA: 1:52 - loss: 5.4661e-04 - acc: 1.000 - ETA: 1:47 - loss: 5.5566e-04 - acc: 1.000 - ETA: 1:44 - loss: 5.8814e-04 - acc: 1.000 - ETA: 1:39 - loss: 0.0033 - acc: 0.9981    - ETA: 1:35 - loss: 0.0037 - acc: 0.998 - ETA: 1:32 - loss: 0.0039 - acc: 0.998 - ETA: 1:27 - loss: 0.0038 - acc: 0.998 - ETA: 1:23 - loss: 0.0037 - acc: 0.998 - ETA: 1:20 - loss: 0.0036 - acc: 0.998 - ETA: 1:16 - loss: 0.0036 - acc: 0.998 - ETA: 1:12 - loss: 0.0036 - acc: 0.998 - ETA: 1:08 - loss: 0.0035 - acc: 0.998 - ETA: 1:03 - loss: 0.0034 - acc: 0.998 - ETA: 1:00 - loss: 0.0034 - acc: 0.998 - ETA: 56s - loss: 0.0033 - acc: 0.998 - ETA: 52s - loss: 0.0033 - acc: 0.99 - ETA: 48s - loss: 0.0032 - acc: 0.99 - ETA: 44s - loss: 0.0031 - acc: 0.99 - ETA: 40s - loss: 0.0031 - acc: 0.99 - ETA: 36s - loss: 0.0031 - acc: 0.99 - ETA: 32s - loss: 0.0030 - acc: 0.99 - ETA: 28s - loss: 0.0030 - acc: 0.99 - ETA: 24s - loss: 0.0029 - acc: 0.99 - ETA: 20s - loss: 0.0029 - acc: 0.99 - ETA: 16s - loss: 0.0028 - acc: 0.99 - ETA: 12s - loss: 0.0028 - acc: 0.99 - ETA: 8s - loss: 0.0028 - acc: 0.9990 - ETA: 4s - loss: 0.0027 - acc: 0.9990Epoch 00010: val_loss improved from 0.00688 to 0.00216, saving model to saved_models/weights.best.iris_Model.hdf5\n",
      "1040/1040 [==============================] - 236s 227ms/step - loss: 0.0027 - acc: 0.9990 - val_loss: 0.0022 - val_acc: 1.0000\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint  \n",
    "\n",
    "\n",
    "epochs = 10\n",
    "\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='saved_models/weights.best.iris_Model.hdf5', \n",
    "                               verbose=1, save_best_only=True)\n",
    "\n",
    "history= model.fit(train_tensors, train_targets, \n",
    "          validation_data=(valid_tensors, valid_targets),\n",
    "          epochs=epochs, batch_size=20, callbacks=[checkpointer], verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Accuracy and Loss Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['val_acc', 'loss', 'acc', 'val_loss'])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x277ba53bd68>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x277ba54e6a0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure()\n",
    "print(history.history.keys())\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "fig.savefig('plot1.png')\n",
    "\n",
    "# summarize history for loss\n",
    "fig1 = plt.figure()\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "fig1.savefig('plot2.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 100.0000%\n"
     ]
    }
   ],
   "source": [
    "# get index of predicted dog Iris for each image in test set\n",
    "iris_predictions = [np.argmax(model.predict(np.expand_dims(tensor, axis=0))) for tensor in test_tensors]\n",
    "\n",
    "# report test accuracy\n",
    "test_accuracy = 100*np.sum(np.array(iris_predictions)==np.argmax(test_targets, axis=1))/len(iris_predictions)\n",
    "print('Test accuracy: %.4f%%' % test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Blurring Task\n",
    "\n",
    "**In the following code cell we add a bluring effect to the testing data to show the strength of our proposed approach**\n",
    "\n",
    "The strength here is that the iris recognition is based on the area of iris that distinguish between persons. If this area is affected by blurring the area of irises my be overlapped. By the way, SVM could be used to classify images, but in this case the probability of error will be very high. Back to our project, In the this section we will evaluate the accuracy in the case of  blurring effect.\n",
    "\n",
    "**Note:** We used the following cell one time to construct the blurring dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "\n",
    "#for i in range(len(test_files)):\n",
    "#    img=cv2.imread(test_files[i])\n",
    "#    img=cv2.blur(img,(7,7))\n",
    "#    img = Image.fromarray(img , 'RGB')\n",
    "#    filename = \"casia/test_blur/file_%i.png\"%i\n",
    "#    #cv2.imwrite(filename, img)\n",
    "#    img.save(filename)\n",
    "test_blur_files, test_blur_targets = load_dataset('casia/test_blur')\n",
    "\n",
    "                          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 260/260 [00:02<00:00, 93.25it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "test_blur_tensors = paths_to_tensor(test_blur_files).astype('float32')/255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the Model in with the blurring effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 100.0000%\n"
     ]
    }
   ],
   "source": [
    "# get index of predicted Iris for each blured image in test set\n",
    "iris_predictions2 = [np.argmax(model.predict(np.expand_dims(tensor, axis=0))) for tensor in test_blur_tensors]\n",
    "\n",
    "# report test accuracy\n",
    "test_accuracy = 100*np.sum(np.array(iris_predictions)==np.argmax(test_targets, axis=1))/len(iris_predictions2)\n",
    "print('Test accuracy: %.4f%%' % test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "**From results:**\n",
    " A\n",
    "* **Our approach is better than the benchmark model as our approach acheived 100 % accuracy and the benchmark about 19 %.\n",
    "\n",
    "* Another point of strength is effeciency under blurring as the proposed model still acheive 100 % accurracy that means it has noise immunity."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dog-project",
   "language": "python",
   "name": "dog-project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
